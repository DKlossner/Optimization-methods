{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 1\n",
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate the implied volatility using the Newton method, I created several functions that correspond to different formualae given in the assignement.\n",
    "\n",
    "I firstly imported the math package that enables the usage of many predefined mathematical functions in Python, for example logarithm function, square root function and exponential function. All of the given values for specific elements  of the option on a stock calculation were written into corresponding variables.\n",
    "\n",
    "The function *d1()* returns the value of the formula $(2)$, given the input parameter sigma as the implied volatility, and using all of the other variables that have already been defined.\n",
    "\n",
    "The function *d2()* similarly, returns the value of the formula  $(3)$, given the input parameter sigma as the implied volatility, and calls the function *d1()* in order to calculate the first element of the subtraction.\n",
    "\n",
    "The function *f()* takes sigma as the input parameter and returns the value obtained using the Black-Sholes option price formula for a (non-dividend-paying) call, which is then reduced by the current value of the option (as given in the formula $(1)$).\n",
    "\n",
    "The function *df()*, again uses the sigma as the input parameter, and is used for the calculation of the derivative of the option price with respect to volatility (the *vega* of the option), given in the formula $(4)$.\n",
    "\n",
    "In the end, the *phi()* function is used to calculate the cumulative distribution function for the standard normal distribution for a given input parameter x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "#current value of the stock\n",
    "S=100\n",
    "#strike of the option\n",
    "K=100\n",
    "#expiration\n",
    "T=1.5\n",
    "#risk-free rate\n",
    "r=0.04\n",
    "#current value of the option\n",
    "option_value = 10.78\n",
    "\n",
    "def d1(sig):\n",
    "    return (math.log(S/K)+(r+0.5*sig**2)*T)/(sig*math.sqrt(T))\n",
    "\n",
    "def d2(sig):\n",
    "    return d1(sig)-sig*math.sqrt(T)\n",
    "\n",
    "#Black-Sholes option price formula reduced by the current value of the option\n",
    "def f(sig):\n",
    "    return S*phi(d1(sig))-K*math.exp(-r*T)*phi(d2(sig)) - option_value\n",
    "\n",
    "#derivative of the option price with respect to volatility\n",
    "def df(sig):\n",
    "    return S*(math.exp(-(d1(sig)**2)/2)/math.sqrt(2*math.pi))*math.sqrt(T)\n",
    "\n",
    "def phi(x):\n",
    "    'Cumulative distribution function for the standard normal distribution'\n",
    "    return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *newton_method()* function is used to implement the Newton method algorithm that is used for the calculation of the root of the function. It accepts four parameters: the function that is observed, it's first derivative, the starting point and the stopping criterium.\n",
    "\n",
    "Current distance from the zero of the function is calculated as the absolute value of the function for the given starting point, and the number of iterations is preserved in the variable *i*.\n",
    "\n",
    "The algorithm keeps on iterating intil the distance becomes lesser than the given stopping criterium. In the meantime, each new point is calculated as the subtraction of the previous point and the fraction of the Black-Scholes function and it's derivative. Afterwards, the updated distance from the root of the function is recalculated and the number of iterations is increased by one.\n",
    "\n",
    "In the end, the obtained results are printed using the *print* function and string formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newtons_method(f, df, x0, e):\n",
    "    #calculate the stopping criterium\n",
    "    delta = abs(f(x0))\n",
    "    #number of iterations\n",
    "    i=0\n",
    "    #keep on iterating until you fullfil the stopping criterium \n",
    "    while delta > e:\n",
    "        #next point is the previous reduced by the value of fraction of the function and it's derivative\n",
    "        x0 = x0 - f(x0)/df(x0)\n",
    "        #recalculate the difference from the stopping criterium\n",
    "        delta = abs(f(x0))\n",
    "        #increase the number of iterations\n",
    "        i+=1\n",
    "    #print the results\n",
    "    print ('Number of iterations is: {}'.format(i))\n",
    "    print ('Root is at: {}'.format(x0))\n",
    "    print ('f(x) at root is: {}'.format(f(x0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented function was tested for the starting point $\\sigma = 1$, and the stoping criterium $\\epsilon = 1^{-5}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations is: 4\n",
      "Root is at: 0.1586478996605295\n",
      "f(x) at root is: 7.85418894366785e-08\n"
     ]
    }
   ],
   "source": [
    "newtons_method(f, df, 1, 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution was reached in only 4 iterations, and the root is found at approximately $\\sigma = 0.159$. The value of the e Black-Scholes option price function reduced by the current value of the option at this point is very close to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "### The Newton method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, I created the funcion *gradientFunc()* that as parameter accepts the array of two values, that represents the point for which the gradient is calculated, and returns the value of the gradient as a numpy matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientFunc(x):\n",
    "    return np.matrix([[4*(x[0]-2)**3+2*(x[0]-2*x[1])],[-4*(x[0]-2*x[1])]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function *hessianFunc()* again accepts the value of a particular point as the input parameter, and returns the matrix with values of the hessian for the given point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessianFunc(x):\n",
    "    return np.matrix([[12*(x[0]-2)**2+2, -4],[-4, 8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function *newton_method()* accepts two input parameters: the starting point and the tolerance level under which the solution can be accepted. It then calculates the starting value for the second norm of the gradient, using the previously mentioned *gradientFunc()* and the predefined numpy function *numpy.linalg.norm*. The variable *i* will be used to follow the number of iterations required to reach a solution. \n",
    "\n",
    "If the starting point is not already within the provided tolerance levels, I will start the calculation of the next point $x_{i+1}$. The previously created functions *hessianFunc()* and *gradientFunc()* are used for the calculation of the Hessian and Gradient for the starting point. The Hessian is then inverted using the predefined numpy function *numpy.linalg.inv*, and then multiplied with the Gradient using the predefined function *numpy.matmul*. Unfortunately, the format of the multiplied matrix is not the same as of the forwarded starting point, so I had to flatten the resulting matrix, transform it into a list and then acces the first element in order to get the vector form. The result is then subtracted from the value of the starting point using a predefined function *numpy.subtract*. Afterwards, the new norm is calculated for the resulting point, and the number of iterations is increased by one.\n",
    "\n",
    "The same steps will keep on iterating in the while loop until the resulting point is close enough to the required minimum, i.e. until the norm gets within the provided tolerance level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_m(x0, e):\n",
    "    #calculate the norm of the gradient\n",
    "    delta = np.linalg.norm(gradientFunc(x0))\n",
    "    #number of iterations\n",
    "    i=0\n",
    "    #while the norm is larger than tolerance level\n",
    "    while delta > e:\n",
    "        #multiply inverted hessian and gradient matrices, then modify them to required vector form\n",
    "        #and subtract from previous point\n",
    "        x0 = np.subtract(x0,np.matmul(np.linalg.inv(hessianFunc(x0)), gradientFunc(x0)).flatten().tolist()[0])\n",
    "        #calculate the new norm\n",
    "        delta = np.linalg.norm(gradientFunc(x0))\n",
    "        #increase the number of iterations\n",
    "        i+=1\n",
    "    #print the results\n",
    "    print ('Number of iterations is: {}'.format(i))\n",
    "    print ('Root is at: {}'.format(x0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then tested the created functions for the starting point $x_0=(3,3)$ and the proposed tolerance level of $10^{-6}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations is: 13\n",
      "Root is at: [2.00513823 1.00256912]\n"
     ]
    }
   ],
   "source": [
    "newton_m([3,3], 10**(-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The root was obtained in 13 iterations, and it's value is approximately $x_{13}=(2.005,1.003)$.\n",
    "\n",
    "The same function was then tested again, just with a different starting point  $x_0=(2,2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-d4603d5bba0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnewton_m\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-0102aa0a9ea1>\u001b[0m in \u001b[0;36mnewton_m\u001b[1;34m(x0, e)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m#multiply inverted hessian and gradient matrices, then modify them to required vector form\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m#and subtract from previous point\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mx0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhessianFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradientFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;31m#calculate the new norm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradientFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36minv\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    526\u001b[0m     \u001b[0msignature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D->D'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'd->d'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m     \u001b[0mainv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Singular matrix\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "newton_m([2,2], 10**(-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this time the function could not obtain the result because the inverse of the Hessian function could not be calculated since it required the division by zero (as the determinant of the matrix becomes equal to zero)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The gradient descent method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent method *gradient_desc_m()*, similarly to the Newton method accepts the starting point and the tolerance level as the input parameters, but it adds the step size (learning rate) as an additional parameter that should be forwarded. \n",
    "\n",
    "It uses the same previously created function named *gradienFunc()* for the gradient and norm calculation, and the variable *i* will again represent the number of iterations of the algorithm. The difference between the two methods lies in the calculation of the calculation of the next point $x_{i+1}$ in each of the iterations. This time, the obtained Gradient value is multiplied with the value of the learning rate and then subtracted from the value of the previous point. The algorithm keeps iterating until it reaches the required tolernce level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_desc_m(x0, s, e):\n",
    "    #calculate the norm of the gradient\n",
    "    delta = np.linalg.norm(gradientFunc(x0))\n",
    "    #number of iterations\n",
    "    i=0\n",
    "    #while the norm is larger than tolerance level\n",
    "    while delta > e:\n",
    "        #multiply the gradient function with the defined learning rate, modify the format\n",
    "        #and subtract from previous point\n",
    "        x0 = np.subtract(x0,(s*gradientFunc(x0)).flatten().tolist()[0])\n",
    "        #calculate the new norm\n",
    "        delta = np.linalg.norm(gradientFunc(x0))\n",
    "        #increase the number of iterations\n",
    "        i+=1\n",
    "    #print the results\n",
    "    print ('Number of iterations is: {}'.format(i))\n",
    "    print ('Root is at: {}'.format(x0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing of the created method was firsly done for the starting point $x_0=(3,3)$, the proposed tolerance level of $10^{-6}$ and the learning rate of $0.18$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations is: 81\n",
      "Root is at: [2.00000722 1.00000351]\n"
     ]
    }
   ],
   "source": [
    "gradient_desc_m([3,3], 0.18, 10**(-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent method reached an already seen root $x_{81}=(2, 1)$ but within 81 iteration.\n",
    "\n",
    "Next the testing of the created method was done for the starting point $x_0=(2,2)$, the tolerance level of $10^{-6}$ and the learning rate of $0.196$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations is: 432\n",
      "Root is at: [1.99999996 1.00000009]\n"
     ]
    }
   ],
   "source": [
    "gradient_desc_m([2,2], 0.196, 10**(-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values of the input parameters helped method reached the root of higher precision than in the previously seen examples, but it required 432 iterations to get to it!\n",
    "\n",
    "I then tried the same values of input parameters for the method, only varying the given step size to $0.2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations is: 18\n",
      "Root is at: [nan inf]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in double_scalars\n",
      "  \n",
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in subtract\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "gradient_desc_m([2,2], 0.2, 10**(-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the method was unable to reach the root with these parameters, and went to infinity within only 18 iterations. This means that the method diverged because the step size was to large.\n",
    "\n",
    "In the end, the step size of $0.15$ was tested, without any changes to the other input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations is: 24354\n",
      "Root is at: [2.00653818 1.00326914]\n"
     ]
    }
   ],
   "source": [
    "gradient_desc_m([2,2], 0.15, 10**(-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It reached the expected root, but this time the method required a significantly larger number of iterations - 24354. This is due to the fact that the step size was too small, so it required many more steps to finally converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
